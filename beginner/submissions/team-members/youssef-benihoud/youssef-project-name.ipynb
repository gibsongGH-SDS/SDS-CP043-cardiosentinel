{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Setup + EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/heart_attack_prediction_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the DataFrame 'df' is loaded from Step 1.\n",
    "\n",
    "# 1. Define the Target Variable (y)\n",
    "TARGET_COLUMN = 'Heart Attack Risk'\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "# 2. Define the Feature Set (X) by dropping the target and non-predictive columns\n",
    "COLUMNS_TO_DROP = ['Patient ID']\n",
    "X = df.drop(columns=[TARGET_COLUMN] + COLUMNS_TO_DROP)\n",
    "\n",
    "# Display the shapes of the resulting datasets to confirm the split\n",
    "print(\"--- X (Features) Shape ---\")\n",
    "print(X.shape)\n",
    "print(\"\\n--- y (Target) Shape ---\")\n",
    "print(y.shape)\n",
    "print(\"\\n--- X (Features) Head (Sample) ---\")\n",
    "print(X.head())\n",
    "\n",
    "# Assuming X (Features) DataFrame is available from Step 2.\n",
    "\n",
    "# 1. Check for Missing Values (Re-check the full feature set X)\n",
    "print(\"--- Missing Values Check (X) ---\")\n",
    "missing_values = X.isnull().sum()\n",
    "# Print only columns with missing values (if any)\n",
    "print(missing_values[missing_values > 0])\n",
    "# If the output is empty, there are no missing values.\n",
    "\n",
    "# 2. Handle the 'Blood Pressure' column by splitting it\n",
    "X[['Systolic BP', 'Diastolic BP']] = X['Blood Pressure'].str.split('/', expand=True)\n",
    "\n",
    "# 3. Convert the new columns to integers and drop the original 'Blood Pressure'\n",
    "X['Systolic BP'] = X['Systolic BP'].astype(int)\n",
    "X['Diastolic BP'] = X['Diastolic BP'].astype(int)\n",
    "X = X.drop('Blood Pressure', axis=1)\n",
    "\n",
    "# Display the head and info of the modified DataFrame to confirm the change\n",
    "print(\"\\n--- X Head After Blood Pressure Split ---\")\n",
    "print(X[['Systolic BP', 'Diastolic BP']].head())\n",
    "print(\"\\n--- X Info After Blood Pressure Split ---\")\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Data Types Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Outliers (domain checks, IQR/z-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Univariate Distributions (histograms, KDE, boxplots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Bivariate Relationships (scatter, groupby stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Target Variable (`y`) Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Class Balance (counts, %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Domain Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Aggregations / Ratios / Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Temporal/Recency Features (if applicable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Feature Documentation (what, why, how)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Imbalance Handling (Preliminary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Strategy Rationale (SMOTE vs. Class Weights vs. Thresholding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Chosen Approach & Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 Sanity Checks (no leakage, applied only to train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Week II–III: Data Preprocessing + Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Train/Validation/Test Split (with stratification if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Create BMI_Category feature ---\n",
    "# Defining standard BMI categories:\n",
    "# < 18.5 (Underweight), 18.5-24.9 (Normal), 25.0-29.9 (Overweight), >= 30.0 (Obese)\n",
    "bins = [0, 18.5, 25.0, 30.0, np.inf]\n",
    "labels = ['Underweight', 'Normal', 'Overweight', 'Obese']\n",
    "\n",
    "X['BMI_Category'] = pd.cut(\n",
    "    X['BMI'],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    right=False,  # Bins include the lower boundary, exclude the upper\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# --- 2. Create Risk_Index feature ---\n",
    "# Formula: (Cholesterol + Systolic BP) / Exercise Hours Per Week\n",
    "# Add a small epsilon to the denominator to prevent division by zero\n",
    "epsilon = 1e-6\n",
    "X['Risk_Index'] = (X['Cholesterol'] + X['Systolic BP']) / (X['Exercise Hours Per Week'] + epsilon)\n",
    "\n",
    "# Display a sample of the new features\n",
    "print(\"--- X Head with new features ---\")\n",
    "print(X[['BMI', 'BMI_Category', 'Systolic BP', 'Exercise Hours Per Week', 'Cholesterol', 'Risk_Index']].head())\n",
    "print(\"\\n--- Value Counts for BMI_Category ---\")\n",
    "print(X['BMI_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Identify Categorical Columns (includes 'object' and 'category' dtypes)\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# 2. Perform One-Hot Encoding\n",
    "# drop_first=True removes the first category level to prevent perfect multicollinearity\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Update X to the encoded DataFrame\n",
    "X = X_encoded\n",
    "\n",
    "# Display the shape and a sample of the new columns\n",
    "print(\"--- X Shape Before Encoding ---\")\n",
    "print(X.shape)\n",
    "print(\"\\n--- X_encoded Shape After Encoding (Notice the increase in columns) ---\")\n",
    "print(X.shape)\n",
    "print(\"\\n--- X_encoded Head (Sample of encoded columns) ---\")\n",
    "# Print a few newly created columns for confirmation\n",
    "print(X[[col for col in X.columns if '_Female' in col or '_Healthy' in col or '_Asia' in col]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Re-attach the target variable y for correlation analysis\n",
    "# (Assuming X and y are available from the previous steps)\n",
    "df_corr = pd.concat([X.copy(), y.rename('Heart Attack Risk')], axis=1)\n",
    "\n",
    "# --- 1. Correlation with Target ---\n",
    "correlation_with_target = df_corr.corr()['Heart Attack Risk'].sort_values(ascending=False)\n",
    "\n",
    "# Select the top 10 most correlated features (excluding the target itself)\n",
    "top_features_for_heatmap = correlation_with_target.drop('Heart Attack Risk').head(10).index.tolist()\n",
    "\n",
    "print(\"--- Top Features Correlated with Heart Attack Risk ---\")\n",
    "print(correlation_with_target[top_features_for_heatmap])\n",
    "\n",
    "# --- 2. Distribution of Key Numerical Features ---\n",
    "numerical_cols = ['Age', 'Cholesterol', 'Systolic BP', 'Heart Rate', 'Risk_Index']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.histplot(df_corr[col], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png')\n",
    "plt.close()\n",
    "\n",
    "# --- 3. Correlation Heatmap ---\n",
    "df_heatmap = df_corr[top_features_for_heatmap + ['Heart Attack Risk']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_heatmap, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Heatmap of Top Features and Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Categorical Encoding (label / one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 1. Identify Numerical Columns\n",
    "# After One-Hot Encoding, all columns should be numerical (int or float)\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "# 2. Instantiate the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 3. Fit and transform the numerical columns\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# 4. Display the head of the scaled columns\n",
    "print(\"--- X Head after Standardization (Showing key numerical features) ---\")\n",
    "print(X[['Age', 'Cholesterol', 'Systolic BP', 'Heart Rate', 'Risk_Index']].head())\n",
    "print(\"\\n--- Mean and Std Dev of Scaled Columns (should be close to 0 and 1) ---\")\n",
    "# Check the mean and std dev for a few key columns to confirm scaling\n",
    "print(X[['Age', 'Cholesterol', 'Systolic BP']].agg(['mean', 'std']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Crucial for maintaining class distribution\n",
    ")\n",
    "\n",
    "# 2. Display the shapes and class distribution\n",
    "print(\"--- Data Splitting Results ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\n--- Target Class Distribution in y_train ---\")\n",
    "# Check the distribution in the training set\n",
    "print(y_train.value_counts(normalize=True).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train, y_train are available from Step 8.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Instantiate SMOTE\n",
    "# Set random_state for reproducibility\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# 2. Apply SMOTE to the training data only\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 3. Display the new shapes and class distribution\n",
    "print(\"--- After SMOTE Resampling ---\")\n",
    "print(f\"Original training set shape (y_train): {y_train.shape}\")\n",
    "print(f\"Resampled training set shape (y_train_res): {y_train_res.shape}\")\n",
    "\n",
    "print(\"\\n--- Target Class Distribution in y_train_res ---\")\n",
    "print(f\"New counts: {Counter(y_train_res)}\")\n",
    "print(f\"Class 0 percentage: {y_train_res.value_counts(normalize=True).loc[0]:.2%}\")\n",
    "print(f\"Class 1 percentage: {y_train_res.value_counts(normalize=True).loc[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Numeric Scaling (StandardScaler / MinMaxScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Building Pipeline (ColumnTransformer + Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Logistic Regression (default settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Decision Tree (default settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Random Forest (default settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Fit → Predict → Evaluate (val set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment Tracking (MLflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 MLflow Setup (tracking URI, experiment name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Log Params, Metrics, Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 GridSearchCV / RandomizedSearchCV (CV strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 Best Params & CV Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 Refit on Train+Val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1 Compare Tuned Models (val/test metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2 Final Choice & Rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Preprocessing Pipeline + Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Week IV: Model Tuning + Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Streamlit App\n",
    "\n",
    "This step must be completed in a separate app.py file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
